import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import os
import json
import sys

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.utils import MODEL_INFO, SAMPLE_METRICS

st.set_page_config(page_title="Analytics - Glaucoma Detection", page_icon="ðŸ“ˆ", layout="wide")

if 'theme' not in st.session_state:
    st.session_state.theme = 'light'

def get_css():
    is_dark = st.session_state.theme == 'dark'
    bg_main = '#0f172a' if is_dark else '#ffffff'
    bg_card = '#1e293b' if is_dark else '#f8fafc'
    text_primary = '#f1f5f9' if is_dark else '#1e293b'
    text_secondary = '#94a3b8' if is_dark else '#64748b'
    border_color = '#475569' if is_dark else '#e2e8f0'
    accent = '#3b82f6'
    
    return f"""
    <style>
        .stApp {{ background-color: {bg_main}; }}
        .page-title {{ color: {accent}; font-size: 2.2rem; font-weight: 700; text-align: center; padding: 1rem 0; border-bottom: 2px solid {border_color}; margin-bottom: 2rem; }}
        .metric-card {{ background: {bg_card}; border: 1px solid {border_color}; border-radius: 12px; padding: 1.5rem; text-align: center; }}
        .metric-value {{ font-size: 2rem; font-weight: 700; color: {accent}; }}
        .metric-label {{ color: {text_secondary}; font-size: 0.9rem; }}
    </style>
    """

st.markdown(get_css(), unsafe_allow_html=True)

st.markdown('<div class="page-title">ðŸ“ˆ Model Analytics Dashboard</div>', unsafe_allow_html=True)

st.info("Note: Displaying sample metrics. Train models with your dataset to see actual performance.")

tab1, tab2, tab3, tab4 = st.tabs(["ðŸ“Š Overview", "ðŸ“ˆ ROC Curves", "ðŸŽ¯ Comparison", "ðŸ“œ History"])

with tab1:
    best_acc = max(SAMPLE_METRICS.items(), key=lambda x: x[1]['accuracy'])
    best_auc = max(SAMPLE_METRICS.items(), key=lambda x: x[1]['roc_auc'])
    best_recall = max(SAMPLE_METRICS.items(), key=lambda x: x[1]['recall'])
    avg_acc = np.mean([m['accuracy'] for m in SAMPLE_METRICS.values()])
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Best Accuracy", f"{best_acc[1]['accuracy']:.1%}", best_acc[0])
    with col2:
        st.metric("Best ROC-AUC", f"{best_auc[1]['roc_auc']:.3f}", best_auc[0])
    with col3:
        st.metric("Best Recall", f"{best_recall[1]['recall']:.1%}", best_recall[0])
    with col4:
        st.metric("Avg Accuracy", f"{avg_acc:.1%}", "All Models")
    
    st.markdown("### All Models Performance")
    
    metrics_df = pd.DataFrame([
        {
            'Model': name,
            'Accuracy': f"{m['accuracy']:.1%}",
            'Precision': f"{m['precision']:.1%}",
            'Recall': f"{m['recall']:.1%}",
            'F1-Score': f"{m['f1_score']:.1%}",
            'ROC-AUC': f"{m['roc_auc']:.3f}"
        }
        for name, m in SAMPLE_METRICS.items()
    ])
    
    st.dataframe(metrics_df, use_container_width=True, hide_index=True)

with tab2:
    st.markdown("### ROC Curves")
    
    selected_roc = st.multiselect(
        "Select models:",
        list(SAMPLE_METRICS.keys()),
        default=['ResNet50', 'DenseNet121', 'EfficientNetB0', 'GlaucoNet']
    )
    
    if selected_roc:
        fig = go.Figure()
        colors = px.colors.qualitative.Plotly
        
        for i, model in enumerate(selected_roc):
            np.random.seed(hash(model) % 2**32)
            auc_val = SAMPLE_METRICS[model]['roc_auc']
            
            fpr = np.sort(np.concatenate([[0], np.random.random(50), [1]]))
            tpr = np.sort(np.concatenate([[0], np.random.random(50) ** (1/auc_val), [1]]))
            
            fig.add_trace(go.Scatter(
                x=fpr, y=tpr,
                name=f'{model} (AUC={auc_val:.3f})',
                mode='lines',
                line=dict(color=colors[i % len(colors)], width=2)
            ))
        
        fig.add_trace(go.Scatter(
            x=[0, 1], y=[0, 1],
            name='Random',
            mode='lines',
            line=dict(dash='dash', color='gray')
        ))
        
        fig.update_layout(
            xaxis_title='False Positive Rate',
            yaxis_title='True Positive Rate',
            height=500
        )
        st.plotly_chart(fig, use_container_width=True)

with tab3:
    st.markdown("### Metrics Comparison")
    
    metric_choice = st.selectbox("Select metric:", 
        ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'sensitivity', 'specificity'])
    
    data = pd.DataFrame([
        {'Model': name, metric_choice: m[metric_choice]}
        for name, m in SAMPLE_METRICS.items()
    ])
    
    fig = px.bar(data, x='Model', y=metric_choice, color=metric_choice, 
                 color_continuous_scale='Blues')
    fig.update_layout(xaxis_tickangle=-45)
    st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("### Radar Chart")
    
    radar_models = st.multiselect("Models for radar:", list(SAMPLE_METRICS.keys()),
        default=['ResNet50', 'DenseNet169', 'GlaucoNet'], key='radar')
    
    if radar_models:
        cats = ['accuracy', 'precision', 'recall', 'f1_score', 'sensitivity', 'specificity']
        fig = go.Figure()
        
        for model in radar_models:
            vals = [SAMPLE_METRICS[model][c] for c in cats] + [SAMPLE_METRICS[model][cats[0]]]
            fig.add_trace(go.Scatterpolar(r=vals, theta=cats + [cats[0]], fill='toself', name=model))
        
        fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 1])))
        st.plotly_chart(fig, use_container_width=True)

with tab4:
    st.markdown("### Training History")
    st.info("Train models to see actual training history. Showing sample curves.")
    
    model_hist = st.selectbox("Select model:", list(MODEL_INFO.keys()))
    
    np.random.seed(hash(model_hist) % 2**32)
    epochs = 30
    
    train_loss = 0.8 - 0.6 * (1 - np.exp(-np.arange(epochs) / 5)) + np.random.normal(0, 0.02, epochs)
    val_loss = 0.85 - 0.55 * (1 - np.exp(-np.arange(epochs) / 5)) + np.random.normal(0, 0.03, epochs)
    train_acc = 0.5 + 0.4 * (1 - np.exp(-np.arange(epochs) / 5)) + np.random.normal(0, 0.02, epochs)
    val_acc = 0.48 + 0.38 * (1 - np.exp(-np.arange(epochs) / 5)) + np.random.normal(0, 0.03, epochs)
    
    fig = make_subplots(rows=1, cols=2, subplot_titles=('Loss', 'Accuracy'))
    
    epoch_list = list(range(1, epochs + 1))
    fig.add_trace(go.Scatter(x=epoch_list, y=train_loss, name='Train Loss'), row=1, col=1)
    fig.add_trace(go.Scatter(x=epoch_list, y=val_loss, name='Val Loss'), row=1, col=1)
    fig.add_trace(go.Scatter(x=epoch_list, y=train_acc, name='Train Acc'), row=1, col=2)
    fig.add_trace(go.Scatter(x=epoch_list, y=val_acc, name='Val Acc'), row=1, col=2)
    
    fig.update_layout(height=400, title=f'{model_hist} Training History (Sample)')
    st.plotly_chart(fig, use_container_width=True)
